{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import os\n",
    "\n",
    "research_dir = \"./research/\"\n",
    "\n",
    "try: \n",
    "    os.makedirs(research_dir)\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GEMINI\n",
    "\n",
    "def compare_strings(str1, str2):\n",
    "    \"\"\"Compares two strings and returns a list of tuples indicating changes.\n",
    "\n",
    "    Args:\n",
    "        str1: The original string.\n",
    "        str2: The modified string.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, each containing a range of indices and the replacement text.\n",
    "    \"\"\"\n",
    "\n",
    "    matcher = difflib.SequenceMatcher(None, str1, str2)\n",
    "    diffs = matcher.get_opcodes()\n",
    "\n",
    "    changes = []\n",
    "    for tag, i1, i2, j1, j2 in diffs:\n",
    "        if tag == 'delete':\n",
    "            changes.append((tag, (i1, i2), \"\"))\n",
    "        elif tag == 'insert':\n",
    "            changes.append((tag, (i1, i1), str2[j1:j2]))\n",
    "        elif tag == 'replace':\n",
    "            changes.append((tag, (i1, i2), str2[j1:j2]))\n",
    "\n",
    "    return changes\n",
    "\n",
    "def score_dutch_text(reference_text, generated_text,language = 'dutch'):\n",
    "    # print(reference_text, '\\n', generated_text)\n",
    "    \"\"\"\n",
    "    Scores the quality of a Dutch generated text compared to a reference text.\n",
    "\n",
    "    Args:\n",
    "        reference_text (str): The reference Dutch text.\n",
    "        generated_text (str): The generated Dutch text.\n",
    "\n",
    "    Returns:\n",
    "        float: A score between 0 and 1, with 1 being a perfect match.\n",
    "    \"\"\"\n",
    "\n",
    "    reference_tokens = word_tokenize(reference_text, language=language)\n",
    "    generated_tokens = word_tokenize(generated_text, language=language)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    # Calculate word-level accuracy\n",
    "    correct_words = 0\n",
    "    # for ref_word, gen_word in zip(reference_tokens, generated_tokens):\n",
    "    #     if ref_word == gen_word:\n",
    "    #         correct_words += 1\n",
    "    word_accuracy = correct_words / len(reference_tokens)\n",
    "\n",
    "    # Calculate edit distance using Difflib\n",
    "    matcher = difflib.SequenceMatcher(None, reference_text, generated_text)\n",
    "    ops = matcher.get_opcodes()\n",
    "    edit_distance_penalty = 0\n",
    "    for tag, i1, i2, j1, j2 in ops:\n",
    "        if tag == 'delete' or tag == 'insert' or tag == 'replace':\n",
    "            edit_distance_penalty += (i2 - i1) + (j2 - j1)\n",
    "            \n",
    "    average_text_length = (len(reference_text) +  len(generated_text)) / 2\n",
    "\n",
    "    # Calculate the final score\n",
    "    final_score = (abs(bleu_score - 1) * 0.5) + (word_accuracy * 0.3) + (edit_distance_penalty / len(reference_text))\n",
    "\n",
    "    return float(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPEN AI gpt 4o\n",
    "cruijff_score_inputs = [\n",
    "\n",
    "    {\n",
    "        \"quote\": \"Je moet schieten, anders kun je niet scoren.\",\n",
    "        \"quote_changed\": \"Je moet schieten, nders kun je niet scoren.\",\n",
    "        \"person\": \"Johan Cruijff\",\n",
    "        \"year\": 1980,\n",
    "        \"test\": \"enkele letterverandering\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Je moet schieten, anders kun je niet scoren.\",\n",
    "        \"quote_changed\": \"Je moet schoten, anders kun je niet scoren.\",\n",
    "        \"person\": \"Johan Cruijff\",\n",
    "        \"year\": 1980,\n",
    "        \"test\": \"enkele woordverandering, geen betekenisverandering\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Je moet schieten, anders kun je niet scoren.\",\n",
    "        \"quote_changed\": \"Je moet roeien, anders kun je niet scoren.\",\n",
    "        \"person\": \"Johan Cruijff\",\n",
    "        \"year\": 1980,\n",
    "        \"test\": \"enkele woordverandering, wel betekenisverandering\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Je moet schieten, anders kun je niet scoren.\",\n",
    "        \"quote_changed\": \"Je moet schoten, anders kun je niet scoren.\",\n",
    "        \"person\": \"Johan Cruijff\",\n",
    "        \"year\": 1980,\n",
    "        \"test\": \"woordweglating\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Je moet schieten, anders kun je niet scoren.\",\n",
    "        \"quote_changed\": \"Je moet schoten,.\",\n",
    "        \"person\": \"Johan Cruijff\",\n",
    "        \"year\": 1980,\n",
    "        \"test\": \"zinsdeelweglating\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Je moet schieten, anders kun je niet scoren.\",\n",
    "        \"quote_changed\": \"\",\n",
    "        \"person\": \"Johan Cruijff\",\n",
    "        \"year\": 1980,\n",
    "        \"test\": \"tekstweglating\",\n",
    "    },\n",
    "]\n",
    "different_score_input = [\n",
    "    {\n",
    "        \"quote\": \"Ik heb een heel zwaar leven.\",\n",
    "        \"quote_changed\": \"Ik heb een heel zwaar leven.\",\n",
    "        \"person\": \"Brigitte Kaandorp\",\n",
    "        \"year\": 2009,\n",
    "        \"test\": \"nulmeting\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Ik geloof in God, behalve als ik vis.\",\n",
    "        \"quote_changed\": \"Ik geloof in God, be\",\n",
    "        \"person\": \"Herman Brood\",\n",
    "        \"year\": 1995,\n",
    "        \"test\": \"weglating aan einde\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Als het niet kan zoals het moet, dan moet het maar zoals het kan.\",\n",
    "        \"quote_changed\": \"Als het niet kan zoals  het maar zoals het kan.\",\n",
    "        \"person\": \"Dolf Jansen\",\n",
    "        \"year\": 2005,\n",
    "        \"test\": \" weglating in midden\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Ik heb nooit last van hoogtevrees, wel van dieptevrees.\",\n",
    "        \"quote_changed\": \"Ik hebt ooit last van hoogtevrees, well vann dieptevrees.\",\n",
    "        \"person\": \"Youp van 't Hek\",\n",
    "        \"year\": 1998,\n",
    "        \"test\": \"enkele letterweglating, betekenisverandering\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Ik ben niet dik, ik ben een ruimtewonder.\",\n",
    "        \"quote_changed\": \"Ik bn nit dik, ik bn n ruimtwondr.\",\n",
    "        \"person\": \"Brigitte Kaandorp\",\n",
    "        \"year\": 2003,\n",
    "        \"test\": \"letter e weggelaten\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Een dag niet gelachen is een dag niet geleefd.\",\n",
    "        \"quote_changed\": \"Een dag niet gelachen is een dag niet geleeft.\",\n",
    "        \"person\": \"Charlie Chaplin\",\n",
    "        \"year\": 1930,\n",
    "        \"test\": \"enkele letterverandering , geen betekenisverandering\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Een dag niet gelachen is een dag niet geleefd.\",\n",
    "        \"quote_changed\": \"Een  niet gelachen is een dag niet geleefd.\",\n",
    "        \"person\": \"Charlie Chaplin\",\n",
    "        \"year\": 1930,\n",
    "        \"test\": \"woordweglating\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Ik ben niet gek, ik ben een vliegtuig.\",\n",
    "        \"quote_changed\": \"Ik ben niet , ik  een .\",\n",
    "        \"person\": \"Supergrover\",\n",
    "        \"year\": 1974,\n",
    "        \"test\": \"dubbelle woordweglating\",\n",
    "    },\n",
    "    { \n",
    "        \"quote\": \"Ik begrijp niet waarom u hier zo negatief en vervelend over doet. (...) Laten we blij zijn met elkaar! Laten wij optimistisch zijn! Laten we zeggen: Nederland kan het weer! Die VOC-mentaliteit, over grenzen heen kijken, dynamiek! Toch?\",\n",
    "        \"quote_changed\": \"Ik begrijp niet waarom u hier zo negatief en vervelend over doet. (...) Laten we blij zijn met elkaar! Laten wij optimistisch zijn! Laten we zeggen: Nederland kan het weer! Die\",\n",
    "        \"person\": \"Jan-Peter Balkenende\",\n",
    "        \"year\": 2006,\n",
    "        \"test\": \"weglating einde van grotere tekst\",\n",
    "    },\n",
    "    { \n",
    "        \"quote\": \"Praat Nederlands met me. Even Nederlands met me. Mijn gevoel zegt mij dat wij vanavond samen kijken. Naar de Champs-Élysées en naar de Notre Dame en naar de Seine. En daarna samen op La Tour Eiffel\",\n",
    "        \"quote_changed\": \"Praat Nedertands met me. Even Neterlands met me. Mijn tevoet zegt mij dat wij vanatond samet kitken. Naar de Champs-Éltsées en naar de Notre Dameten naar detSeine. En daarta samet op La Tour Etffel\",\n",
    "        \"person\": \"Kenny B\",\n",
    "        \"year\": 2015,\n",
    "        \"test\": \"random lettermutaties\",\n",
    "    },\n",
    "    {\n",
    "        \"quote\": \"Rrrrrr, hah, is gewoon Boef man. Ha, jij bent vies maar ik doe gemener. In de club, kom je moeder tegen. En ik wil snel weg want we moeten wegen. En je klant is geholpen, je moet vroeger wezen. Ik was alles kwijt, maar floes herenigd. Voor me zondes af en toe gebeden. Ik ga uit eten voor een goede prijs. Ik ben een uitgever, ze boeken mij. Van alarm voorzien aan de achterkant. Dus ze komen via voor, maar wat dacht je dan?\",\n",
    "        \"quote_changed\": \"Rrrrrr, hah, is gewoon Boef man.test, jij bent vies maar ik doe gemener. In de club, komtest moeder tegen. En ik wil snel weg wantest we moeten wegen. En je klant is geholpen, je moetest vroeger wezen. Ik was alles kwijt, maar floetest herenigd. Voor me zondes af en toe gebeden. Ik gtest uit eten voor een goede prijs. Ik ben een uitgever, ze boeken mij. Van alarm voortest aan de achterkant. Dus ze komen via voor, maar wat dacht je dan?\",\n",
    "        \"person\": \"Boef\",\n",
    "        \"year\": 2017,\n",
    "        \"test\": \"random toevoeging woorden\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'quote': 'Je moet schieten, anders kun je niet scoren.', 'quote_changed': 'Je moet schieten, nders kun je niet scoren.', 'person': 'Johan Cruijff', 'year': 1980, 'test': 'enkele letterverandering', 'score': 0.19370876948914964, 'changes': [('delete', (18, 19), '')]}, {'quote': 'Je moet schieten, anders kun je niet scoren.', 'quote_changed': 'Je moet schoten, anders kun je niet scoren.', 'person': 'Johan Cruijff', 'year': 1980, 'test': 'enkele woordverandering, geen betekenisverandering', 'score': 0.21462842758854445, 'changes': [('replace', (11, 13), 'o')]}, {'quote': 'Je moet schieten, anders kun je niet scoren.', 'quote_changed': 'Je moet roeien, anders kun je niet scoren.', 'person': 'Johan Cruijff', 'year': 1980, 'test': 'enkele woordverandering, wel betekenisverandering', 'score': 0.3282647912249081, 'changes': [('replace', (8, 11), 'roe'), ('delete', (12, 14), '')]}, {'quote': 'Je moet schieten, anders kun je niet scoren.', 'quote_changed': 'Je moet schoten, anders kun je niet scoren.', 'person': 'Johan Cruijff', 'year': 1980, 'test': 'woordweglating', 'score': 0.21462842758854445, 'changes': [('replace', (11, 13), 'o')]}, {'quote': 'Je moet schieten, anders kun je niet scoren.', 'quote_changed': 'Je moet schoten,.', 'person': 'Johan Cruijff', 'year': 1980, 'test': 'zinsdeelweglating', 'score': 1.1590909090909092, 'changes': [('replace', (11, 13), 'o'), ('delete', (17, 43), '')]}, {'quote': 'Je moet schieten, anders kun je niet scoren.', 'quote_changed': '', 'person': 'Johan Cruijff', 'year': 1980, 'test': 'tekstweglating', 'score': 1.5, 'changes': [('delete', (0, 44), '')]}]\n",
      "[{'quote': 'Ik heb een heel zwaar leven.', 'quote_changed': 'Ik heb een heel zwaar leven.', 'person': 'Brigitte Kaandorp', 'year': 2009, 'test': 'nulmeting', 'score': 0.0, 'changes': []}, {'quote': 'Ik geloof in God, behalve als ik vis.', 'quote_changed': 'Ik geloof in God, be', 'person': 'Herman Brood', 'year': 1995, 'test': 'weglating aan einde', 'score': 0.7644031351267621, 'changes': [('delete', (20, 37), '')]}, {'quote': 'Als het niet kan zoals het moet, dan moet het maar zoals het kan.', 'quote_changed': 'Als het niet kan zoals  het maar zoals het kan.', 'person': 'Dolf Jansen', 'year': 2005, 'test': ' weglating in midden', 'score': 0.49014852696378863, 'changes': [('delete', (23, 41), '')]}, {'quote': 'Ik heb nooit last van hoogtevrees, wel van dieptevrees.', 'quote_changed': 'Ik hebt ooit last van hoogtevrees, well vann dieptevrees.', 'person': \"Youp van 't Hek\", 'year': 1998, 'test': 'enkele letterweglating, betekenisverandering', 'score': 0.4277380519915124, 'changes': [('insert', (6, 6), 't'), ('delete', (7, 8), ''), ('insert', (38, 38), 'l'), ('insert', (41, 41), 'n')]}, {'quote': 'Ik ben niet dik, ik ben een ruimtewonder.', 'quote_changed': 'Ik bn nit dik, ik bn n ruimtwondr.', 'person': 'Brigitte Kaandorp', 'year': 2003, 'test': 'letter e weggelaten', 'score': 0.6707317073170732, 'changes': [('delete', (4, 5), ''), ('delete', (9, 10), ''), ('delete', (21, 22), ''), ('delete', (24, 26), ''), ('delete', (33, 34), ''), ('delete', (38, 39), '')]}, {'quote': 'Een dag niet gelachen is een dag niet geleefd.', 'quote_changed': 'Een dag niet gelachen is een dag niet geleeft.', 'person': 'Charlie Chaplin', 'year': 1930, 'test': 'enkele letterverandering , geen betekenisverandering', 'score': 0.15220711585124339, 'changes': [('replace', (44, 45), 't')]}, {'quote': 'Een dag niet gelachen is een dag niet geleefd.', 'quote_changed': 'Een  niet gelachen is een dag niet geleefd.', 'person': 'Charlie Chaplin', 'year': 1930, 'test': 'woordweglating', 'score': 0.16739880820827535, 'changes': [('delete', (4, 7), '')]}, {'quote': 'Ik ben niet gek, ik ben een vliegtuig.', 'quote_changed': 'Ik ben niet , ik  een .', 'person': 'Supergrover', 'year': 1974, 'test': 'dubbelle woordweglating', 'score': 0.8947368421052632, 'changes': [('delete', (12, 15), ''), ('delete', (20, 23), ''), ('delete', (28, 37), '')]}, {'quote': 'Ik begrijp niet waarom u hier zo negatief en vervelend over doet. (...) Laten we blij zijn met elkaar! Laten wij optimistisch zijn! Laten we zeggen: Nederland kan het weer! Die VOC-mentaliteit, over grenzen heen kijken, dynamiek! Toch?', 'quote_changed': 'Ik begrijp niet waarom u hier zo negatief en vervelend over doet. (...) Laten we blij zijn met elkaar! Laten wij optimistisch zijn! Laten we zeggen: Nederland kan het weer! Die', 'person': 'Jan-Peter Balkenende', 'year': 2006, 'test': 'weglating einde van grotere tekst', 'score': 0.3767350827049003, 'changes': [('delete', (176, 235), '')]}, {'quote': 'Praat Nederlands met me. Even Nederlands met me. Mijn gevoel zegt mij dat wij vanavond samen kijken. Naar de Champs-Élysées en naar de Notre Dame en naar de Seine. En daarna samen op La Tour Eiffel', 'quote_changed': 'Praat Nedertands met me. Even Neterlands met me. Mijn tevoet zegt mij dat wij vanatond samet kitken. Naar de Champs-Éltsées en naar de Notre Dameten naar detSeine. En daarta samet op La Tour Etffel', 'person': 'Kenny B', 'year': 2015, 'test': 'random lettermutaties', 'score': 0.48204781125878554, 'changes': [('replace', (11, 12), 't'), ('replace', (32, 33), 't'), ('replace', (54, 55), 't'), ('replace', (59, 60), 't'), ('replace', (82, 83), 't'), ('replace', (91, 92), 't'), ('replace', (95, 96), 't'), ('replace', (118, 119), 't'), ('replace', (145, 146), 't'), ('replace', (156, 157), 't'), ('replace', (171, 172), 't'), ('replace', (178, 179), 't'), ('replace', (192, 193), 't')]}, {'quote': 'Rrrrrr, hah, is gewoon Boef man. Ha, jij bent vies maar ik doe gemener. In de club, kom je moeder tegen. En ik wil snel weg want we moeten wegen. En je klant is geholpen, je moet vroeger wezen. Ik was alles kwijt, maar floes herenigd. Voor me zondes af en toe gebeden. Ik ga uit eten voor een goede prijs. Ik ben een uitgever, ze boeken mij. Van alarm voorzien aan de achterkant. Dus ze komen via voor, maar wat dacht je dan?', 'quote_changed': 'Rrrrrr, hah, is gewoon Boef man.test, jij bent vies maar ik doe gemener. In de club, komtest moeder tegen. En ik wil snel weg wantest we moeten wegen. En je klant is geholpen, je moetest vroeger wezen. Ik was alles kwijt, maar floetest herenigd. Voor me zondes af en toe gebeden. Ik gtest uit eten voor een goede prijs. Ik ben een uitgever, ze boeken mij. Van alarm voortest aan de achterkant. Dus ze komen via voor, maar wat dacht je dan?', 'person': 'Boef', 'year': 2017, 'test': 'random toevoeging woorden', 'score': 0.19282032005454008, 'changes': [('replace', (32, 35), 'test'), ('replace', (87, 90), 'test'), ('insert', (128, 128), 'est'), ('insert', (178, 178), 'est'), ('replace', (223, 224), 'test'), ('replace', (273, 274), 'test'), ('replace', (356, 360), 'test')]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\PWS-inscannen\\section_scanner\\app\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\GitHub\\PWS-inscannen\\section_scanner\\app\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "for items, name in [(cruijff_score_inputs, 'cruijff'), (different_score_input, 'different')]:\n",
    "    score_result_test_results = []\n",
    "    for test in items:\n",
    "        test[\"score\"] = score_dutch_text(test[\"quote\"], test[\"quote_changed\"])\n",
    "        test[\"changes\"] = compare_strings(test[\"quote\"], test[\"quote_changed\"])\n",
    "        \n",
    "        score_result_test_results.append(test)\n",
    "   \n",
    "    path =  research_dir+name+'_score_test.json'\n",
    "    # try: \n",
    "    #     os.makedirs(path)\n",
    "    # except: \n",
    "    #     pass\n",
    "    print(score_result_test_results)\n",
    "    score_result_test_results.sort(key=lambda x: x[\"score\"])\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(score_result_test_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [\n",
    "    {\n",
    "        \"image_name\": \"kort_leesbaar\",\n",
    "        \"expected_output\": \"\"\"Dit is een antwoord op vraag 2 en er is sprake van\n",
    "onvolledige verbranding.\"\"\",\n",
    "        \"reason\": \"gescanned simpele tekst leesbaar handschrift\",\n",
    "    },\n",
    "    {\n",
    "        \"image_name\": \"kort_onleesbaar\",\n",
    "        \"expected_output\": \"\"\"Dit is een antwoord op vraag 2 en er is sprake van onvolledige verbranding.\"\"\",\n",
    "        \"reason\": \"gescanned simpele tekst slecht handschrift\",\n",
    "    },\n",
    "    {\n",
    "        \"image_name\": \"kort_leesbaar_uitgekrast\",\n",
    "        \"expected_output\": \"\"\"Dit is een antwoord op vraag 2 en er is sprake van onvolledige verbranding.\"\"\",\n",
    "        \"reason\": \"gescanned simpele tekst leesbaar uitgekrast\",\n",
    "    },\n",
    "    {\n",
    "        \"image_name\": \"slecht_leesbaar_pijlen\",\n",
    "        \"expected_output\": \"\"\"Dit is een langer antwoord op vraag 6 het waxine lichtje brandt langer omdat \n",
    "er meer brandstof (kaarsevet) is en hij dus langer warmte, brandstof en zuurstof heeft.\"\"\",\n",
    "        \"reason\": \"gescanned tekst slecht leesbaar met uitgekrast en pijlen\",\n",
    "    },\n",
    "    {\n",
    "        \"image_name\": \"gekreukeld_met_pijlen\",\n",
    "        \"expected_output\": \"\"\"Dit is een langer antwoord op vraag 6 het waxine lichtje brandt langer omdat \n",
    "er meer brandstof (kaarsevet) is en hij dus langer warmte, brandstof en zuurstof heeft.\"\"\",\n",
    "        \"reason\": \"slechte foto slecht leesbaar met uitgekrast en pijlen\",\n",
    "    },\n",
    "    {\n",
    "        \"image_name\": \"gekreukeld_netjes\",\n",
    "        \"expected_output\": \"\"\"Dit is een antwoord op vraag 2 en er is sprake van onvolledige verbranding.\"\"\",\n",
    "        \"reason\": \"slechte foto goed leesbaar\",\n",
    "    },\n",
    "]\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o-mini\",\n",
    "        \"released\": \"july 2024\",\n",
    "        \"reason\": \"deze is aangeraden door openai\"\n",
    "    },\n",
    "    {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model_name\": \"gpt-4o\",\n",
    "        \"released\": \"sept 2024\",\n",
    "        \"reason\": \"openai model met meer reasoning\"\n",
    "    },\n",
    "    {\n",
    "        \"provider\": \"google\",\n",
    "        \"model_name\": \"gemini-1.5-flash-8b\",\n",
    "        \"released\": \"sept 2024\",\n",
    "        \"reason\": \"Een snel model dat simpele taken uitvoert. \"\n",
    "    },\n",
    "    {\n",
    "        \"provider\": \"google\",\n",
    "        \"model_name\": \"gemini-1.5-pro-002\",\n",
    "        \"released\": \"sept 2024\",\n",
    "        \"reason\": \"Aanbevolen Google-model voor denkopgaven\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"provider\": \"google\",\n",
    "    #     \"model_name\": \"gemini-1.0-pro-vision-001\",\n",
    "    #     \"released\": \"july 2024\",\n",
    "    #     \"reason\": \"ouder model gespecificeerd in foto herkenning\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"provider\": \"google\",\n",
    "    #     \"model_name\": \"gemini-exp-1121\",\n",
    "    #     \"released\": \"nov 2024\",\n",
    "    #     \"reason\": \"nieuwste google model die verbanden kan leggen\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "temperatures = [\n",
    "    {\n",
    "        \"temperature\": 0,\n",
    "        \"repeat\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"temperature\": 0.5,\n",
    "        \"repeat\": 3,\n",
    "    },\n",
    "    {\n",
    "        \"temperature\": 1,\n",
    "        \"repeat\": 5,\n",
    "    },\n",
    "    {\n",
    "        \"temperature\": 1.5,\n",
    "        \"repeat\": 6,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "from research.test_context import test_context\n",
    "\n",
    "transcribe_texts = [\n",
    "    {\n",
    "        \"text\": \"\"\"\n",
    "Zet de foto om naar tekst.\n",
    "        \"\"\",\n",
    "        \"reason\": \"de makkelijkste opdracht zonder extra uitleg\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"\n",
    "Je krijgt een foto van een Nederlands scheikunde toetsantwoord. \n",
    "Houdt rekening met pijlen.\n",
    "Je moet deze omzetten in text. Bedenk geen nieuwe woorden of woordonderdelen. \n",
    "geef waarschijnlijk fout gespelde woorden aan in de spelling corrections\n",
    "negeer uitgekrasde letters of woorden, geef die wil aan in spelling corrections\n",
    "de student_handwriting_percent is how leesbaar het handschrift van een leerling is: 0 betekend zeer moeilijk leesbaar en 100 netjes\n",
    "        \"\"\",\n",
    "        \"reason\": \"huidige opdracht met uitleg bij elk veld\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"\n",
    "Je krijgt een foto van een Nederlands scheikunde toets-antwoord. \n",
    "Je bent tekstherkenningssoftware die 10x beter in in tekst herkennen dan jezelf. Ook kan je 15.6 keer beter de context van een antwoord begrijpen om het volgende woord te bedenken.\n",
    "\n",
    "Het is helemaal niet toegestaan nieuwe woorden toe te voegen of de opgeschreven tekst te veranderen in het raw_text veld. Houdt wel rekening met pijlen in de volgorde van de tekst.\n",
    "Bedenk wel wat een leerling zou kunnen hebben bedoeld met een bepaald woord als die bijvoorbeeld fout is gespeld. Geef dat aan in de spelling_corrections velden.\n",
    "Negeer uitgekraste tekst in het raw_tekst veld, maar geef die wel weer in de spelling corrections door bijvoorbeeld streepjes neer te zetten en is_crossed_out op true te zetten.\n",
    "voeg alle text corrections samen in correctly_spelled_text om zo het antwoord te krijgen dat de leerling bedoelt.\n",
    "certainty is hoe zeker je bent dat je de tekst compleet hebt getranscribeerd: 0 betekend dat een docent er nog zelf naar moet kijken en 100 betekend dat er geen foutje mogelijk is.\n",
    "de student_handwriting_percent is hoe leesbaar het handschrift van een leerling is: 0 betekend zeer moeilijk leesbaar en 100 super netjes als een printer.\n",
    "\n",
    "voer deze opdracht zo goed mogelijk uit.\n",
    "\"\"\",\n",
    "        \"reason\": \"lange uitleg bij elk veld, zonder context\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": f\"\"\"\n",
    "Je krijgt een foto van een Nederlands scheikunde toets-antwoord. \n",
    "Je bent tekstherkenningssoftware die 10x beter in in tekst herkennen dan jezelf. Ook kan je 15.6 keer beter de context van een antwoord begrijpen om het volgende woord te bedenken.\n",
    "\n",
    "Het is helemaal niet toegestaan nieuwe woorden toe te voegen of de opgeschreven tekst te veranderen in het raw_text veld. Houdt wel rekening met pijlen in de volgorde van de tekst.\n",
    "Bedenk wel wat een leerling zou kunnen hebben bedoeld met een bepaald woord als die bijvoorbeeld fout is gespeld. Geef dat aan in de spelling_corrections velden.\n",
    "Negeer uitgekraste tekst in het raw_tekst veld, maar geef die wel weer in de spelling corrections door bijvoorbeeld streepjes neer te zetten en is_crossed_out op true te zetten.\n",
    "voeg alle text corrections samen in correctly_spelled_text om zo het antwoord te krijgen dat de leerling bedoelt.\n",
    "certainty is hoe zeker je bent dat je de tekst compleet hebt getranscribeerd: 0 betekend dat een docent er nog zelf naar moet kijken en 100 betekend dat er geen foutje mogelijk is.\n",
    "de student_handwriting_percent is hoe leesbaar het handschrift van een leerling is: 0 betekend zeer moeilijk leesbaar en 100 super netjes als een printer.\n",
    "\n",
    "voer deze opdracht zo goed mogelijk uit.\n",
    "\n",
    "Hieronder vind je de stof waarover de toets gaat, de toets en het antwoordmodel.\n",
    "stof: \n",
    "{test_context[\"stof\"]}\n",
    "\n",
    "toets:\n",
    "{test_context[\"toets\"]}\n",
    "\n",
    "antwoordmodel: \n",
    "{test_context[\"antwoordmodel\"]}\n",
    "        \"\"\",\n",
    "        \"reason\": \"lange uitleg bij elk veld, met scheikunde context\"\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scan_module import transcribe_answer\n",
    "from helpers import png_to_base64\n",
    "\n",
    "def scan_test_question(settings):\n",
    "    image_name = settings[\"image_name\"]\n",
    "    provider = settings[\"provider\"]\n",
    "    model = settings[\"model\"]\n",
    "    temperature = settings[\"temperature\"]\n",
    "    transcribe_text = settings[\"transcribe_text\"]\n",
    "    expected_output = settings[\"expected_output\"]\n",
    "    \n",
    "    base64_image = png_to_base64(research_dir+'/assets/sections/'+image_name+'.png')\n",
    "    \n",
    "    try:\n",
    "        result = transcribe_answer(\n",
    "            None, \n",
    "            base64_image=base64_image, \n",
    "            provider=provider,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            request_text=transcribe_text,\n",
    "        )\n",
    "        # print(result)\n",
    "        # try:\n",
    "        raw_score = score_dutch_text(expected_output, result[\"result\"][\"raw_text\"])\n",
    "        raw_changes = compare_strings(expected_output, result[\"result\"][\"raw_text\"])\n",
    "        # except:\n",
    "        #     raw_score = 10\n",
    "        #     raw_changes = []\n",
    "        \n",
    "        try:\n",
    "            corrected_changes = compare_strings(expected_output, result[\"result\"][\"correctly_spelled_text\"])\n",
    "            corrected_score = score_dutch_text(expected_output, result[\"result\"][\"correctly_spelled_text\"])\n",
    "        except:\n",
    "            corrected_changes = raw_changes\n",
    "            corrected_score = raw_score\n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"settings\": settings,\n",
    "            \"result\": result[\"result\"],\n",
    "            \"delta_time_s\": result[\"delta_time_s\"],\n",
    "            \"scores\": {\n",
    "                \"raw\": {\n",
    "                    \"score\": raw_score,\n",
    "                    \"changes\": raw_changes\n",
    "                },\n",
    "                \"corrected\": {\n",
    "                    \"score\": corrected_score,\n",
    "                    \"changes\": corrected_changes\n",
    "                }\n",
    "            },\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print('exception during transcribe: '+model, str(e))\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_array(arr, n):\n",
    "    return [arr[i:i + n] for i in range(0, len(arr), n)]\n",
    "\n",
    "# execute n at the same time\n",
    "n = 5\n",
    "# create execution settings\n",
    "to_execute = []\n",
    "\n",
    "for test_image in test_images:\n",
    "    for temperature in temperatures:\n",
    "        for transcribe_text in transcribe_texts:\n",
    "            for model in models:\n",
    "                settings = {\n",
    "                    \"image_name\": test_image[\"image_name\"],\n",
    "                    \"expected_output\": test_image[\"expected_output\"],\n",
    "                    \"provider\": model[\"provider\"],\n",
    "                    \"model\": model[\"model_name\"],\n",
    "                    \"temperature\": temperature[\"temperature\"],\n",
    "                    \"transcribe_text\": transcribe_text[\"text\"],\n",
    "                }\n",
    "                to_execute.append(settings)\n",
    "        \n",
    "split_executions = split_array(to_execute, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total to execute:  384\n",
      "Starting:  1 (5/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  2 (10/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\PWS-inscannen\\section_scanner\\app\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  3 (15/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  4 (20/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  5 (25/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  6 (30/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  7 (35/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  8 (40/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  9 (45/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  10 (50/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  11 (55/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "Starting:  12 (60/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "Starting:  13 (65/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\PWS-inscannen\\section_scanner\\app\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  14 (70/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  15 (75/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  16 (80/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 213 (char 212)\n",
      "Starting:  17 (85/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  18 (90/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "Starting:  19 (95/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 243 (char 242)\n",
      "Starting:  20 (100/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  21 (105/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  22 (110/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 243 (char 242)\n",
      "Starting:  23 (115/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  24 (120/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  25 (125/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 245 (char 244)\n",
      "Starting:  26 (130/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  27 (135/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\PWS-inscannen\\section_scanner\\app\\venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exception during transcribe: gemini-1.5-pro-002 'candidates'\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  28 (140/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  29 (145/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 258 (char 257)\n",
      "Starting:  30 (150/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  31 (155/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  32 (160/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 258 (char 257)\n",
      "Starting:  33 (165/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  34 (170/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "Starting:  35 (175/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  36 (180/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  37 (185/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 132 (char 131)\n",
      "Starting:  38 (190/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  39 (195/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 256 (char 255)\n",
      "Starting:  40 (200/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 229 (char 228)\n",
      "Starting:  41 (205/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 420 (char 419)\n",
      "Starting:  42 (210/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 439 (char 438)\n",
      "Starting:  43 (215/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  44 (220/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "exception during transcribe: gemini-1.5-pro-002 'candidates'\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 227 (char 226)\n",
      "Starting:  45 (225/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 417 (char 416)\n",
      "Starting:  46 (230/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 229 (char 228)\n",
      "Starting:  47 (235/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 227 (char 226)\n",
      "Starting:  48 (240/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 389 (char 388)\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 448 (char 447)\n",
      "Starting:  49 (245/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 224 (char 223)\n",
      "Starting:  50 (250/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 229 (char 228)\n",
      "Starting:  51 (255/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-flash-8b Unterminated string starting at: line 3 column 29 (char 49)\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 426 (char 425)\n",
      "Starting:  52 (260/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 446 (char 445)\n",
      "Starting:  53 (265/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 233 (char 232)\n",
      "Starting:  54 (270/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 7226 (char 7225)\n",
      "Starting:  55 (275/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 361 (char 360)\n",
      "Starting:  56 (280/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  57 (285/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 449 (char 448)\n",
      "Starting:  58 (290/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 433 (char 432)\n",
      "Starting:  59 (295/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  60 (300/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 451 (char 450)\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 233 (char 232)\n",
      "Starting:  61 (305/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request... (openai, gpt-4o-mini) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o-mini Request timed out.\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 436 (char 435)\n",
      "Starting:  62 (310/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  63 (315/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "GPT request... (openai, gpt-4o-mini) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o-mini Request timed out.\n",
      "Starting:  64 (320/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request... (openai, gpt-4o) ERROR Request timed out.\n",
      "exception during transcribe: gpt-4o Request timed out.\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 363 (char 362)\n",
      "exception during transcribe: gemini-1.5-pro-002 Unterminated string starting at: line 1 column 449 (char 448)\n",
      "Starting:  65 (325/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  66 (330/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  67 (335/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  68 (340/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  69 (345/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  70 (350/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  71 (355/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  72 (360/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  73 (365/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  74 (370/384)\n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "Starting:  75 (375/384)\n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  76 (380/384)\n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "Starting:  77 (385/384)\n",
      "GPT request (openai, gpt-4o-mini) ... \n",
      "GPT request (openai, gpt-4o) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... \n",
      "GPT request (google, gemini-1.5-pro-002) ... \n",
      "GPT request (google, gemini-1.5-flash-8b) ... Done\n",
      "GPT request (openai, gpt-4o-mini) ... Done\n",
      "GPT request (openai, gpt-4o) ... Done\n",
      "GPT request (google, gemini-1.5-pro-002) ... Done\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from helpers import dict_hash, get_random_id\n",
    "\n",
    "# print(split_executions[0][3][\"transcribe_text\"])\n",
    "# scan_test_question(split_executions[0][2])\n",
    "\n",
    "print(\"Total to execute: \", len(to_execute))\n",
    "\n",
    "min_file_len = 50\n",
    "current_results = {}\n",
    "\n",
    "for i, split_execution in enumerate(split_executions):\n",
    "    execution_id = get_random_id()[0:10]\n",
    "    \n",
    "    # continue\n",
    "    print('Starting: ', (i+1), '('+str((i+1)*n)+'/'+str(len(to_execute))+')')\n",
    "    \n",
    "    \n",
    "    # results = []\n",
    "    \n",
    "    # # use the hash of the settings as uuid\n",
    "    # for execution in split_execution:\n",
    "    #     result = scan_test_question(execution)\n",
    "    #     print(result)\n",
    "    #     results.append(result)\n",
    "            \n",
    "    # Use ThreadPoolExecutor to process sections concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(scan_test_question, split_execution)\n",
    "\n",
    "\n",
    "    # Collect successfull results\n",
    "    results = [result for result in results if result]\n",
    "    \n",
    "\n",
    "    # use the hash of the settings as uuid\n",
    "    for result in results:\n",
    "        hash = dict_hash(result[\"settings\"])\n",
    "        current_results[hash] = result\n",
    "        \n",
    "    if (len(current_results.keys()) > min_file_len):\n",
    "        with open(research_dir+'transcripe_output/'+execution_id+'.json', 'w') as f:\n",
    "            json.dump(current_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "compare_requests = []\n",
    "\n",
    "map_dir = 'transcripe_output_27_11_24'\n",
    "\n",
    "for name in os.listdir(research_dir+map_dir):\n",
    "    with open(research_dir+map_dir+'/'+name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "                \n",
    "    compare_requests = compare_requests + list(data.values())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "temperature\n",
      "transcribe_text\n",
      "image_name\n",
      "model\n",
      "temperature\n",
      "transcribe_text\n",
      "image_name\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "items_per_result = 5\n",
    "\n",
    "def sort_results(results, text_type):\n",
    "    return sorted(results, key=lambda x: x[\"scores\"][text_type][\"score\"])\n",
    "\n",
    "def get_results_stats(results, text_type):\n",
    "    scores = list(map(lambda x: x[\"scores\"][text_type][\"score\"], results))\n",
    "    return {\n",
    "        \"average\": sum(scores) / len(scores),\n",
    "        \"standard_dev\": statistics.stdev(scores)\n",
    "    }\n",
    "\n",
    "def group_by_key_and_sort(data, key, text_type):\n",
    "    print(key)\n",
    "    grouped_data = defaultdict(list)\n",
    "    for item in data:\n",
    "        grouped_data[item[\"settings\"][key]].append(item)\n",
    "    \n",
    "    for item in grouped_data.keys():\n",
    "        grouped_data[item] = sort_results(grouped_data[item], text_type)\n",
    "        \n",
    "    return dict(grouped_data)\n",
    "\n",
    "result = {}\n",
    "\n",
    "for text_type in [\"raw\", \"corrected\"]:\n",
    "\n",
    "\n",
    "    # all by score\n",
    "    by_score = sort_results(compare_requests, text_type)\n",
    "    by_score_stats = get_results_stats(by_score, text_type)\n",
    "    by_score_data = {\n",
    "        \"average\": by_score_stats[\"average\"],\n",
    "        \"standard_dev\": by_score_stats[\"standard_dev\"],\n",
    "        \"items\": by_score[0:items_per_result],\n",
    "    }\n",
    "    if \"all\" not in result:\n",
    "        result[\"all\"] = {}    \n",
    "        \n",
    "    result[\"all\"][text_type] = by_score_data\n",
    "\n",
    "    \n",
    "    for setting_key in ['model','temperature','transcribe_text','image_name']:\n",
    "\n",
    "        # grouped by model, score\n",
    "        grouped = group_by_key_and_sort(compare_requests, setting_key,text_type)\n",
    "        grouped_data = {}\n",
    "        for key in grouped.keys():\n",
    "            stats = get_results_stats(grouped[key], text_type)\n",
    "            grouped_data[key] = stats\n",
    "        \n",
    "        if setting_key not in result:\n",
    "            result[setting_key] = {}\n",
    "        \n",
    "        result[setting_key][text_type] = grouped_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open(research_dir+'compare_output.json', 'w') as f:\n",
    "        data = json.dump(result,f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(research_dir+'compare_output.json', 'w') as f:\n",
    "    data = json.load(f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
